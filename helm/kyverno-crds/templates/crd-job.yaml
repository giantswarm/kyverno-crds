apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Release.Name }}-install-job
  namespace: {{ .Release.Namespace | quote }}
  annotations:
    # create hook dependencies in the right order
    "helm.sh/hook-weight": "-1"
  labels:
    {{- include "labels.common" . | nindent 4 }}
spec:
  template:
    metadata:
      labels:
        {{- include "labels.common" . | nindent 8 }}
    spec:
      serviceAccountName: {{ .Release.Name }}
        {{- with .Values.podSecurityContext }}
      securityContext:
          {{- . | toYaml | nindent 8 }}
        {{- end }}
      tolerations:
      - key: node-role.kubernetes.io/control-plane
        effect: NoSchedule
      containers:
      - name: kubectl
        image: "{{ default (.Values.global).image.registry .Values.image.registry }}/giantswarm/kubectl:{{ .Values.image.tag }}"
        command:
        - sh
        - -c
        - |
          set -o errexit ; set -o xtrace ; set -o nounset

          # piping stderr to stdout means kubectl's errors are surfaced
          # in the pod's logs.
          kubectl apply --force-conflicts --server-side -f /data/ 2>&1
        volumeMounts:
{{- range $path, $_ := .Files.Glob "crd/**" }}
        - name: {{ $path | base | trimSuffix ".yaml" }}
          mountPath: /data/{{ $path | base }}
          subPath: {{ $path | base }}
{{- end }}
        {{- with .Values.securityContext }}
        securityContext:
          {{- . | toYaml | nindent 10 }}
        {{- end }}
        resources: {{- toYaml .Values.resources | nindent 10 }}
      volumes:
{{- $currentScope := . }}
{{- range $path, $_ := .Files.Glob "crd/**" }}
    {{- with $currentScope }}
      - name: {{ $path | base | trimSuffix ".yaml" }}
        configMap:
          name: {{ .Release.Name }}-{{ $path | base | trimSuffix ".yaml" }}
          items:
          - key: content
            path: {{ $path | base }}
{{- end }}
{{- end }}
      restartPolicy: Never
  backoffLimit: 4
